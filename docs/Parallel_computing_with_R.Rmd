---
title: "The Good, The Bad, The Ugly with R"
knit: (function(input_file, encoding) {
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), 'index.html'))})
author: "Ehsan Jahanpour"
date: "12/08/2019"
output:
  html_document: 
    toc: true
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(doParallel)
library(foreach)
library(reshape2)
num_cores <- parallel::detectCores() 
registerDoParallel(num_cores)
```

## Introduction

Once upon a time in Hollywood, it was adequate for statisticians, data analysts, physicians, ... to

a. pull up the Rstudio, 
b. `read.csv()` the file, 
c. for loop into the files and 
d. apply statistical analytics and draw inference from data and experiments. 

Since then, the universe has been expanding and so the programming languages and methods. With great power comes great responsibilities (challanges) though. The good methodology in the past is getting really ugly nowadays without us noticing that. Thus, for us (as reasearchers, data enthusiasts, scientists, ...) there is modern need to understand the nature of the problem, the quality/size of the data in hand, philosophy, pros and cons of different methods. Well, the end goal is to select the best tool to solve those problems, also never forget *Correlation does not ALWAYS imply causations, though sometimes it does*. I am writing this blog trying to change our programming habbits with the hope "Make Data Analysis Great Again!". I will try my best to explain why, when, and how to apply parallel computing


![](../images/Picture1.png)

## Three / Two / One / Action (Data Import)

Following the cinematic schema of the blog, I would like to use movie data to illustrate how parallelization can be usefull to make our lives less dramatic. The dataset I am using here is downloaded from [CMU Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz). They stated in their page that there are 42,306 rows of plot summaries availanle in the txt file. For more information about the dataset please visit the page. The plot summary of the movies are stored in `plto_summaries.txt` file. Let's take a look at the first two lines of the table:

```{r movies}
# plot_summary = read.csv('../sample_plot_summmary.csv')
system.time(plot_summary <- read.table('../data/plot_summaries.txt', sep = '\t'))
cat(paste('read.table with sep, table dimension', list(dim(plot_summary)), 'Incorrect and slow- Ugly'))
system.time(plot_summary <- read.delim2('../data/plot_summaries.txt', sep = '\t'))
cat(paste('read.delim2 with sep, table dimension', list(dim(plot_summary)), 'Incorrect and slow - Ugly'))
system.time(plot_summary <- read.table('../data/plot_summaries.txt', sep = '\t', quote = ''))
cat(paste('using read.table with quote options, table dimension', list(dim(plot_summary)), 'Correct but slow - Bad'))
system.time(plot_summary <- data.table::fread('../data/plot_summaries.txt', header = FALSE, quote = ''))
cat(paste('using fread with quote options, table dimension', list(dim(plot_summary)), 'Correct and Fast - Good'))
```

Ok, as you can see there are a lot of Good, bad and ugly way to read in the data from data source. especially when you are dealing with real and dirty data. The take home here is `DO NOT NEGLECT EOF within quoted string` warning as it might result in not reading the whole data into your program. if you would like to find out why, read the paragraph below, otherwise feel free to skip that.

*optional* But what does 'EOF within quoted string' statement tell us and why we are seeing three different row numbers for tables? they are all due to quotations!! (either single or double quotes). In a perfect world, you would close any quote that you "opened". But, hey, we are not leaving in a perfect world! So it is possible to open a quotation mark and not close it. or is `'` in `John's book` a quote? No at all. So, in first `read.table` the default quotes are `' and "`, thus, if the function sees one of those marks in a row, it will look for another quotation mark to close it and if could not find it (number of quotation marks are odd), it will go to the next row to match. That causes lots of rows to be considered as on row and only gets rows instead of 42,306 rows. Same story happens with `read.delim2`, the only difference is the default quote for `?read.delim2` is only `"` that reduce the chance of unclosed quotation marks and we see more rows appear (The Uglies). Changing the `quote` argument will fix the issue and reduce the running time by 3rd, great tha we are now pulling data correctly with better with better performance! But wait!! 10 seconds (The Bad)?!! can we do better? Thanks to the `fread` function of `data.table` package, we can speed up data reading 76 times faster (The Good).

## Plot Summary

```{r plot_summaries}
plot_summary <- as.data.frame(plot_summary)
row_number <- seq(10000, nrow(plot_summary), by= 10000)
knitr::kable(head(plot_summary, n=2L)) 
```
As the table above shows, the table includes the movie id and plot summary. Using some imagination and creativity, there are lots of cool things we can do from those plot summaries, such as grouping the movies with similar subjects, finding a movie about a specific topic for a Sunday afternoon, categorizing the movies (re-inventing the wheel!! not so great), training a model to create a new plot. Well, that might be interesting to see a movie that is written by an AI model. However, for all those cool stuffs, we need to read in the data and extract some features out of them. For simplicity, let's suppose I naively think **world war** and **Johannesburg** are THE two keywords going to help my model with clustering the movies along with number of words in the summary plot!! (well, this was, literally, the techniques during pre-deep learning era to "create" features from text). *extract_text_features* function is generating those features from a text:


```{r function}
extract_text_features <- function(text){
  words_to_search <- c('johannesburg', 'world war')
  reg_word <- paste0(words_to_search, collapse = '|')
  word_exists <- grepl(reg_word, tolower(text))
  word_count <- sapply(strsplit(tolower(text), " "), length)
  return(c(word_exists, word_count))
}
```

## Exposure (Feature Extraction from Movie Plots)

So far, we have created a function that extract "our" features on a single text. Now, it is time to go over each row in dataframe and apply the function on plot summaries. This is one of the major applications of the loop in any programming language. So, the first option comes to any mind is using for loop. However, this could get really ugly. As another take home message *TRY TO AVOID "FOR LOOPS" IN R AS MUCH AS YOU CAN, unless you are familiar with primitive functions and stuff*. I will briefly explain it in the next paragraph, but if you would like to skip it, please feel free to do so.

*optional* According to Hadley Wickham's (Advanced R)[http://adv-r.had.co.nz/memory.html#modification] "For loops in R have a reputation for being slow. Often that slowness is because you’re modifying a copy instead of modifying in place." The function below is applying *for loops* on a dataframe. Since dataframe is quasi-built-in type of data in R, and not a primitive function developed in C, each time for loop is being compiled, a new COPY of sample is created and `keyword and word_counts` columns are updated (Thanks to updates on R 3.1.0. the copy is not deep anymore). Imaging you are building a wall from scratch in a new place anytime you would like to add or change a brick on it. That makes the function really slow and ugly.

```{r ugly}
for_on_dataframe <- function(sample) {
    for (i in 1:nrow(sample))
      sample[i, c('keyword', 'word_counts')] <- extract_text_features(sample[i, 2])
    return(sample)
}
``` 

As discussed above, using for loop on dataframes could really get upgly (as your data size increases) mainly due to running codes in R and creating unnecessary objects. However, we might be able to improve the performance through changing `data.frame` to `list` or using apply family functions. The apply functions in R don't provide improved performance over other looping functions (e.g. for). It is mentioned in some resources that lapply can be a little faster because it does more work in C code than in R. The chunk below includes 2 functions from apply family: `lapply` and `mapply (a multivariate form of sapplt)` and for loop on list item. The performance improved on large data sources though there is not a significant difference between those functions. All those functions are better than the ugly "for on large dataframe" function though they are only using one of the cores and still The bad. Well, if you do not run R on a device with a single-core!! CPU. 

```{r bad}
lapply_script <- function(sample) {
  sample[, c('keyword', 'word_count')] <- do.call(rbind, lapply(sample[[2]], extract_text_features))
  return(sample)
}

mapply_script <- function(sample) {
  sample[, c('keyword', 'word_count')] <- mapply(extract_text_features, sample[, 2])
  return(sample)
}

for_on_list <- function(sample) {
  sample <- as.list(sample)
  for (i in 1:length(sample[[1]])) {
    temp <- extract_text_features(sample[[2]][i])
    sample[['keyword']][i] <- temp[1]
    sample[['word_count']][i] <- temp[2]
  }
}
``` 

Each of feature extraction tasks in the for loop is independant, thus, we might be able to speed up the process through parallelizing the process. If you would like to dig deeper in the parallilzation, please check out this awesome [page](http://www.parallelr.com/r-with-parallel-computing/). The code chunk below is using `doParallel` library to implement parallel code that runs locally on a single multi-processor (multi-cores) computer. The package also provides a parallel backend for the `foreach` package. in spite of it's great advantages, parallelizing R processes have some challanges that we need to keep in mind.  


I think, there are two main caveats in using parallel processes: a) mapping the task among multiple cores and collecting the results back usually take time and work. We would get a substantial speed up if the task (`extract_text_features` function in this example) takes enough time for the overhead to be worth it. Also, accessing global variables and dealing with global state will necessarily be different than single thread execution. As each processor will get a copy of the vector that we’ve passed in, if we modify the data inplace, it will simply be modifying the copy instead of the original vector. To overcome data handling issues within cores, we should set the `foreach` in a way that it returns the information from the parallelized task:

>      extracts <- foreach::foreach (i = 1:nrow(sample), .combine = rbind) %dopar%
>      extract_text_features(sample[i, 2])


```{r good}
parallel_loop_script <- function(sample) {
    doParallel::registerDoParallel(num_cores)
    extracts <- foreach::foreach (i = 1:nrow(sample), .combine = rbind) %dopar%
      extract_text_features(sample[i, 2])
    return(as.data.frame(extracts))
  }
``` 

### Why parallel computing?

Now, Let's go ahead and assess the performance of each loop executaion. We categorized the loops into The Good 

```{r timeline}

# If foreach is better let's change the behavior and use it while we can
runtime_list <- foreach (i = 1:length(row_number)) %dopar%
{
  for_dataframe <- system.time(for_on_dataframe(plot_summary[1:row_number[i],]))
  lapply_method <- system.time(lapply_script(plot_summary[1:row_number[i],]))
  mapply_method <- system.time(mapply_script(plot_summary[1:row_number[i],]))
  for_list <- system.time(for_on_list(plot_summary[1:row_number[i],])) 
  for_parrallel <- system.time(parallel_loop_script(plot_summary[1:row_number[i],]))
  return(list(i, for_dataframe[[3]], lapply_method[[3]], mapply_method[[3]], for_list[[3]], for_parrallel[[3]]))
}
```


```{r visualization}
# change the list of lists to dataframe
runtime_df <- do.call(rbind.data.frame, runtime_list)
names(runtime_df) <- c('index', 'for_dataframe', 'lapply', 'mapply', 'for_list', 'parallel_loop')

# need to melt the dataframe to be able to line plot them
melted <- reshape2::melt(runtime_df, measure.vars = c('for_dataframe', 'lapply', 'mapply', 'for_list', 'parallel_loop'))
ggplot(melted, aes(index, value, color = variable)) +
  geom_line(aes(group = paste0(variable)))
# x <- 1:length(run_time1)
# plot(x, run_time1, type = 'l', col = 'blue')
# lines(x, run_time2, type = 'l', col = 'red')
# lines(x, run_time3, type = 'l', col = 'green')
# legend(x=1.5, y=1.5, legend=c("for", "mapply", 'dopar'), col = c('blue', 'red', 'green'), lty=1)

```

### When to Parallelize?

we don’t want to parallelizing things that already runs really fast sequentially. In other words, it would be better to take advantage of other techniques (primitive functions, vectorization, data.table packages) before parallelizing the process.

So when working on code parallelization it is important to keep in mind that for very short jobs, the parallelization overhead will diminish the parallelization benefits. If we’re not sure if the parallelization of specific piece of code improves its speed, we can always check the execution time using system.time().

When we pass in our data to run a parallel task, each processors will get a copy of the data, thus it’s not going to modify the global/intial data in any way.

Vectorization, writing C++ function using RCPP or using parallel format of apply family (like mcapply) or are other options to speed up the execution time in R. I did not include them here. I might end up writing another blog post for those options later.



## Parallel packages in R

(This Link)[https://cran.r-project.org/web/views/HighPerformanceComputing.html] lists the packages being developed for high performance computation in R. if you enjoyed the topic and would like to learn more about it, please visit the cran page.  